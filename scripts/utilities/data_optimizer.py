"""
‚ö° Sistema de Otimiza√ß√£o de Dados para Dashboard LULC
====================================================

Este m√≥dulo implementa otimiza√ß√µes avan√ßadas para processamento
de dados fixos, eliminando recomputa√ß√µes desnecess√°rias.

Features:
- Pr√©-processamento inteligente
- Dados agregados otimizados
- √çndices pr√©-computados
- Transforma√ß√µes em lote
- Compress√£o autom√°tica

Author: Sistema de Otimiza√ß√£o Dashboard LULC
Date: 2025-07-22
"""

import pandas as pd
from typing import Dict, Any, Tuple, Optional
import streamlit as st
from pathlib import Path
import json
from .cache_manager import cached, get_cache_manager

class DataOptimizer:
    """
    üöÄ Otimizador de Dados para Performance M√°xima
    
    Pr√©-processa e otimiza dados fixos para eliminar
    recomputa√ß√µes em tempo de execu√ß√£o.
    """
    
    def __init__(self):
        """Inicializa o otimizador."""
        self.cache_manager = get_cache_manager()
        self._optimized_data: Dict[str, Any] = {}
        
    @cached(ttl_seconds=7200, persist=True, key_prefix="optimized_")
    def optimize_initiatives_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        üéØ Otimiza dados de iniciativas com agrega√ß√µes pr√©-computadas.
        
        Args:
            df: DataFrame com dados das iniciativas
            
        Returns:
            Dicion√°rio com dados otimizados e agrega√ß√µes
        """
        optimized = {}
        
        # ===== AGREGA√á√ïES B√ÅSICAS =====
        optimized['total_initiatives'] = len(df)
        optimized['unique_sensors'] = df['Sensor'].nunique() if 'Sensor' in df.columns else 0
        optimized['unique_countries'] = df['Country'].nunique() if 'Country' in df.columns else 0
        
        # ===== DADOS CATEG√ìRICOS OTIMIZADOS =====
        if 'Sensor' in df.columns:
            sensor_counts = df['Sensor'].value_counts().to_dict()
            optimized['sensor_distribution'] = sensor_counts
            optimized['top_sensors'] = list(sensor_counts.keys())[:10]
        
        if 'Type' in df.columns:
            type_counts = df['Type'].value_counts().to_dict()
            optimized['type_distribution'] = type_counts
        
        if 'Country' in df.columns:
            country_counts = df['Country'].value_counts().to_dict()
            optimized['country_distribution'] = country_counts
            optimized['top_countries'] = list(country_counts.keys())[:15]
        
        # ===== AN√ÅLISES TEMPORAIS PR√â-COMPUTADAS =====
        date_columns = [col for col in df.columns if 'date' in col.lower() or 'year' in col.lower()]
        
        for date_col in date_columns:
            if date_col in df.columns:
                try:
                    # Converte para datetime se necess√°rio
                    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):
                        df_temp = df.copy()
                        df_temp[date_col] = pd.to_datetime(df_temp[date_col], errors='coerce')
                    else:
                        df_temp = df
                    
                    # Extrai ano para an√°lises
                    df_temp['year'] = df_temp[date_col].dt.year
                    year_counts = df_temp['year'].value_counts().sort_index().to_dict()
                    optimized[f'{date_col}_yearly'] = year_counts
                    
                    # Estat√≠sticas temporais
                    optimized[f'{date_col}_range'] = {
                        'min': df_temp[date_col].min().isoformat() if pd.notna(df_temp[date_col].min()) else None,
                        'max': df_temp[date_col].max().isoformat() if pd.notna(df_temp[date_col].max()) else None
                    }
                except Exception:
                    continue
        
        # ===== AN√ÅLISES GEOGR√ÅFICAS =====
        geo_columns = ['Country', 'Region', 'Location', 'Continent']
        for geo_col in geo_columns:
            if geo_col in df.columns:
                optimized[f'{geo_col.lower()}_stats'] = {
                    'unique_count': df[geo_col].nunique(),
                    'top_5': df[geo_col].value_counts().head(5).to_dict()
                }
        
        # ===== DADOS PARA FILTROS OTIMIZADOS =====
        optimized['filter_options'] = {}
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns
        
        for col in categorical_columns:
            if col in df.columns and df[col].nunique() < 100:  # Evita colunas com muitos valores √∫nicos
                unique_values = sorted(df[col].dropna().unique().tolist())
                optimized['filter_options'][col] = unique_values
        
        # ===== M√âTRICAS DE QUALIDADE DOS DADOS =====
        optimized['data_quality'] = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'missing_data_percent': (df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100),
            'columns_with_missing': df.isnull().any().sum(),
            'duplicate_rows': df.duplicated().sum()
        }
        
        # ===== DADOS PARA TABELAS PAGINADAS =====
        optimized['paginated_data'] = self._create_paginated_chunks(df)
        
        return optimized
    
    def _create_paginated_chunks(self, df: pd.DataFrame, chunk_size: int = 50) -> Dict[int, pd.DataFrame]:
        """Cria chunks paginados dos dados."""
        chunks = {}
        total_pages = (len(df) + chunk_size - 1) // chunk_size
        
        for page in range(total_pages):
            start_idx = page * chunk_size
            end_idx = min((page + 1) * chunk_size, len(df))
            chunks[page] = df.iloc[start_idx:end_idx].copy()
        
        return chunks
    
    @cached(ttl_seconds=7200, persist=True, key_prefix="charts_")
    def optimize_chart_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        üìä Pr√©-computa dados para gr√°ficos otimizados.
        
        Args:
            df: DataFrame com dados das iniciativas
            
        Returns:
            Dados otimizados para gr√°ficos
        """
        chart_data = {}
        
        # ===== DADOS PARA GR√ÅFICO DE BARRAS (SENSORES) =====
        if 'Sensor' in df.columns:
            sensor_data = df['Sensor'].value_counts().head(15)
            chart_data['sensor_bar_chart'] = {
                'labels': sensor_data.index.tolist(),
                'values': sensor_data.values.tolist(),
                'total': sensor_data.sum()
            }
        
        # ===== DADOS PARA GR√ÅFICO DE PIZZA (TIPOS) =====
        if 'Type' in df.columns:
            type_data = df['Type'].value_counts()
            chart_data['type_pie_chart'] = {
                'labels': type_data.index.tolist(),
                'values': type_data.values.tolist()
            }
        
        # ===== DADOS PARA GR√ÅFICO TEMPORAL =====
        date_columns = [col for col in df.columns if 'date' in col.lower() or 'year' in col.lower()]
        
        for date_col in date_columns:
            if date_col in df.columns:
                try:
                    df_temp = df.copy()
                    if not pd.api.types.is_datetime64_any_dtype(df_temp[date_col]):
                        df_temp[date_col] = pd.to_datetime(df_temp[date_col], errors='coerce')
                    
                    df_temp['year'] = df_temp[date_col].dt.year
                    yearly_data = df_temp['year'].value_counts().sort_index()
                    
                    chart_data[f'{date_col}_timeline'] = {
                        'years': yearly_data.index.tolist(),
                        'counts': yearly_data.values.tolist()
                    }
                except Exception:
                    continue
        
        # ===== DADOS PARA GR√ÅFICO GEOGR√ÅFICO =====
        if 'Country' in df.columns:
            country_data = df['Country'].value_counts().head(20)
            chart_data['geographic_chart'] = {
                'countries': country_data.index.tolist(),
                'counts': country_data.values.tolist()
            }
        
        return chart_data
    
    @cached(ttl_seconds=3600, persist=True, key_prefix="search_")
    def optimize_search_indices(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        üîç Cria √≠ndices otimizados para busca e filtros.
        
        Args:
            df: DataFrame com dados das iniciativas
            
        Returns:
            √çndices otimizados para busca
        """
        indices = {}
        
        # ===== √çNDICE DE TEXTO COMPLETO =====
        text_columns = ['Name', 'Description', 'Organization', 'Country']
        searchable_text = []
        
        for idx, row in df.iterrows():
            text_parts = []
            for col in text_columns:
                if col in df.columns and pd.notna(row[col]):
                    text_parts.append(str(row[col]).lower())
            
            searchable_text.append(' '.join(text_parts))
        
        indices['searchable_text'] = searchable_text
        indices['row_indices'] = df.index.tolist()
        
        # ===== √çNDICES CATEG√ìRICOS =====
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns
        
        for col in categorical_columns:
            if col in df.columns:
                # Cria mapeamento de valor -> lista de √≠ndices
                value_to_indices = {}
                for idx, value in enumerate(df[col]):
                    if pd.notna(value):
                        value_str = str(value).lower()
                        if value_str not in value_to_indices:
                            value_to_indices[value_str] = []
                        value_to_indices[value_str].append(idx)
                
                indices[f'{col}_index'] = value_to_indices
        
        return indices
    
    def get_optimized_data(self, key: str) -> Any:
        """Recupera dados otimizados do cache."""
        return self.cache_manager.get(f"optimized_{key}")
    
    def invalidate_optimization(self, pattern: Optional[str] = None):
        """Invalida otimiza√ß√µes baseado em padr√£o."""
        if pattern:
            self.cache_manager.invalidate(f"optimized_{pattern}")
        else:
            self.cache_manager.invalidate("optimized_")

# Inst√¢ncia global do otimizador
_data_optimizer = None

def get_data_optimizer() -> DataOptimizer:
    """Retorna inst√¢ncia singleton do otimizador."""
    global _data_optimizer
    if _data_optimizer is None:
        _data_optimizer = DataOptimizer()
    return _data_optimizer

@cached(ttl_seconds=3600, persist=True, key_prefix="processed_")
def load_and_optimize_initiatives(file_path: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    üöÄ Carrega e otimiza dados de iniciativas em uma opera√ß√£o.
    
    Args:
        file_path: Caminho para o arquivo de dados
        
    Returns:
        Tupla com DataFrame e dados otimizados
    """
    # Carrega dados
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    df = pd.DataFrame(data)
    
    # Otimiza dados
    optimizer = get_data_optimizer()
    optimized_data = optimizer.optimize_initiatives_data(df)
    chart_data = optimizer.optimize_chart_data(df)
    search_indices = optimizer.optimize_search_indices(df)
    
    # Combina todas as otimiza√ß√µes
    full_optimization = {
        'basic_stats': optimized_data,
        'chart_data': chart_data,
        'search_indices': search_indices
    }
    
    return df, full_optimization

@cached(ttl_seconds=1800, persist=True, key_prefix="sensors_")
def load_and_optimize_sensors(file_path: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    üì° Carrega e otimiza dados de sensores.
    
    Args:
        file_path: Caminho para o arquivo de sensores
        
    Returns:
        Tupla com DataFrame e dados otimizados
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    df = pd.DataFrame(data)
    
    # Otimiza√ß√µes espec√≠ficas para sensores
    optimizations = {
        'total_sensors': len(df),
        'sensor_types': df['type'].value_counts().to_dict() if 'type' in df.columns else {},
        'resolution_stats': df['resolution'].value_counts().to_dict() if 'resolution' in df.columns else {},
        'unique_platforms': df['platform'].nunique() if 'platform' in df.columns else 0
    }
    
    return df, optimizations

def preload_all_data():
    """
    üéØ Pr√©-carrega todos os dados otimizados na inicializa√ß√£o.
    
    Esta fun√ß√£o deve ser chamada uma vez na inicializa√ß√£o do app
    para garantir que todos os dados estejam otimizados e em cache.
    """
    try:
        # Caminhos dos arquivos
        initiatives_file = "data/initiatives_metadata.jsonc"
        sensors_file = "data/sensors_metadata.jsonc"
        
        if Path(initiatives_file).exists():
            st.info("üöÄ Pr√©-carregando dados de iniciativas...")
            load_and_optimize_initiatives(initiatives_file)
            
        if Path(sensors_file).exists():
            st.info("üì° Pr√©-carregando dados de sensores...")
            load_and_optimize_sensors(sensors_file)
            
        # Mostra estat√≠sticas do cache
        cache_stats = get_cache_manager().get_stats()
        st.success(f"‚úÖ Dados otimizados carregados! Hit Rate: {cache_stats['hit_rate_percent']}%")
        
    except Exception as e:
        st.error(f"‚ùå Erro no pr√©-carregamento: {e}")

def display_optimization_stats():
    """Exibe estat√≠sticas de otimiza√ß√£o no Streamlit."""
    st.subheader("‚ö° Estat√≠sticas de Otimiza√ß√£o")
    
    optimizer = get_data_optimizer()
    cache_manager = get_cache_manager()
    
    # Estat√≠sticas do cache
    stats = cache_manager.get_stats()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Cache Hit Rate", f"{stats['hit_rate_percent']}%")
        st.metric("Entradas em Mem√≥ria", stats['memory_entries'])
    
    with col2:
        st.metric("Uso de Mem√≥ria", f"{stats['memory_usage_mb']:.1f}MB")
        st.metric("Entradas em Disco", stats['disk_entries'])
    
    with col3:
        st.metric("Compress√µes", stats['compressions'])
        st.metric("Evictions", stats['evictions'])
    
    # Gr√°fico de performance
    if stats['hits'] + stats['misses'] > 0:
        cache_data = pd.DataFrame({
            'Tipo': ['Cache Hits', 'Cache Misses'],
            'Quantidade': [stats['hits'], stats['misses']]
        })
        
        st.bar_chart(cache_data.set_index('Tipo'))
    
    # Bot√µes de controle
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üîÑ Recarregar Otimiza√ß√µes"):
            optimizer.invalidate_optimization()
            preload_all_data()
            st.rerun()
    
    with col2:
        if st.button("üóëÔ∏è Limpar Cache Completo"):
            cache_manager.invalidate()
            st.success("Cache limpo!")
            st.rerun()
